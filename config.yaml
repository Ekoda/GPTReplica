hardware:
  device: cuda 

data:
  tokenizer: gpt2
  train_val_ratio: 0.9

model:
  model_dimensions: 256 # GPT-3 uses 128 * layers
  layers: 2
  attention_heads: 4

training:
  batch_size: 16
  sequence_length: 64
  n_epochs: 2
  learning_rate: 1e-4
  optimizer: Adam
  loss_function: CrossEntropyLoss
  dropout_rate: 0.2
  grad_clip: off

parameters:
  load:
    should_load: True
    path: ../params/test_256.pth
  save:
    should_save: True
    path: ../params/test_256.pth