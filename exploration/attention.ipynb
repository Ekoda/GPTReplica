{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi query attention component went under several iterations, starting with an additional head class which was discarded for a simpler \"one class\" implementation with increased performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    '''\n",
    "    Multi Query Attention Layer as described in the paper \"Fast Transformer Decoding: One Write-Head is All You Need\" (Shazeer, 2019)\n",
    "\n",
    "    In Multi Query Attention, the keys and values are shared across attention heads, reducing the memory required at inference time at the cost of a small decrease in performance as compared to multi-head attention.\n",
    "    The aim being to reduce the memory bandwidth requirements.\n",
    "\n",
    "    https://arxiv.org/abs/1911.02150\n",
    "\n",
    "    '''\n",
    "    def __init__(self, model_dimension: int, n_heads: int, dropout: float, mask: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dimension = model_dimension // n_heads\n",
    "        self.mask = mask\n",
    "        self.keys = nn.Linear(model_dimension, self.head_dimension, bias=False)\n",
    "        self.values = nn.Linear(model_dimension, self.head_dimension, bias=False)\n",
    "        self.queries = nn.ModuleList([nn.Linear(model_dimension, self.head_dimension, bias=False) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(model_dimension, model_dimension, bias=False)\n",
    "        self.dropout_p = dropout\n",
    "        self.r_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        K = self.keys(X)\n",
    "        V = self.values(X)\n",
    "        heads = [F.scaled_dot_product_attention(query(X), K, V, dropout_p=self.dropout_p, is_causal=self.mask) for query in self.queries]\n",
    "        concat = torch.cat(heads, dim=-1)\n",
    "        linear = self.linear(concat)\n",
    "        return self.r_dropout(linear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unhappy with the performance so I went back to the books and discovered batching and other techniques which improved performance significantly at the cost of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    '''\n",
    "    Multi Query Attention Layer as described in the paper \"Fast Transformer Decoding: One Write-Head is All You Need\" (Shazeer, 2019)\n",
    "\n",
    "    In Multi Query Attention, the keys and values are shared across attention heads, reducing the memory required at inference time at the cost of a small decrease in performance as compared to multi-head attention.\n",
    "    The aim being to reduce the memory bandwidth requirements.\n",
    "\n",
    "    https://arxiv.org/abs/1911.02150\n",
    "\n",
    "    '''\n",
    "    def __init__(self, model_dimension: int, n_heads: int, dropout: float, mask: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dimension = model_dimension // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.mask = mask\n",
    "        self.queries = nn.Linear(model_dimension, model_dimension, bias=False)\n",
    "        self.kv_projection = nn.Linear(model_dimension, self.head_dimension * 2, bias=False)\n",
    "        self.linear = nn.Linear(model_dimension, model_dimension, bias=False)\n",
    "        self.dropout_p = dropout\n",
    "        self.r_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        batch_len, seq_len, embd_dim = X.shape\n",
    "        Q = self.queries(X).view(batch_len, seq_len, self.n_heads, self.head_dimension).transpose(1, 2) # (batch, n_heads, seq_len, head_dimension)\n",
    "\n",
    "        # Notes on KV projection:\n",
    "        # - The key and value projections were initially seperated but here we combine them into a single projection to reduce the number of linear layers.\n",
    "        # - As a result we need to split the output of the projection into two tensors, one for keys and one for values.\n",
    "        # - We also need to match the shape of the queries tensor by adding a head dimension.\n",
    "        # - Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size\n",
    "        K, V = self.kv_projection(X).unsqueeze(1).expand(-1, self.n_heads, -1, -1).split(self.head_dimension, dim=-1)\n",
    "\n",
    "        heads = F.scaled_dot_product_attention(Q, K, V, dropout_p=self.dropout_p, is_causal=self.mask)\n",
    "\n",
    "        # Notes on concat:\n",
    "        # - With this method we also need a new way of concating the heads back together\n",
    "        # - instead of concating the heads as the previous version we instead reshape the heads tensor to match the shape of the input tensor\n",
    "        # - view requires the new shape to be contiguous, so we call contiguous before calling view\n",
    "        concat = heads.transpose(1, 2).contiguous().view(batch_len, seq_len, embd_dim)\n",
    "        \n",
    "        linear = self.linear(concat)\n",
    "        return self.r_dropout(linear)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
